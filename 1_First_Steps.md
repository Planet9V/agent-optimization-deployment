To get OpenSPG fully running locally in Octoberâ€¯2025, after completing the model configuration you mentioned, you need to complete four main setup stages: starting the core services, creating a knowledge project, loading data, and enabling reasoning/query execution. The following reflects the current OpenSPGâ€¯0.8â€¯workflow and toolchain.â€‹

ğŸ”§ 1. Start the OpenSPG Core Engine
If you already ran Docker successfully:

bash
docker-compose -f docker-compose.yml up -d
Ensure all four containers are up:

openspg-server â†’ manages schema, rules, and APIs

openspg-mysql â†’ stores schema and metadata

tugraph â†’ graph data backend

elasticsearch â†’ index for retrieval

Then confirm that the web frontend is live at:

text
http://127.0.0.1:8887
You should see the visual schema editor and service dashboard.â€‹

ğŸ§° 2. Set Up the KAG (Knowledge Augmented Generation) Toolkit
OpenSPGâ€™s reasoning and querying layer is provided via KAG (a companion package for semantic reasoning and LLMâ€¯integration).

Install KAG inside a Pythonâ€¯3.10+â€¯virtual environment:

bash
git clone https://github.com/OpenSPG/KAG.git
cd KAG
pip install -e .
Verify installation:

bash
knext --version
You should see something like knext 0.8.0.â€‹

ğŸ§© 3. Configure a Knowledge Project
Now, define your local Knowledgeâ€¯Base (KB) that binds together vectorizers, models, and data.

Inside the folder KAG/examples, copy and edit a config file:

bash
cp examples/example_config.yaml my_config.yaml
vim my_config.yaml
Example configuration:

text
openie_llm:
  api_key: sk-<your-openai-key>
  base_url: https://api.openai.com/v1
  model: gpt-5-mini
  type: maas

vectorizer:
  api_key: sk-<your-openai-key>
  base_url: https://api.openai.com/v1
  model: text-embedding-3-small
  type: openai
  vector_dimensions: 1536

project:
  id: "1"
  host_addr: http://127.0.0.1:8887
  language: en
  namespace: my_local_kb
Then initialize the project:

bash
cd examples
knext project create --prj_path .
knext schema commit
ğŸ“¥ 4. Import and Vectorize Data
Upload or import knowledge sources (CSV,â€¯TXT,â€¯JSONL):

bash
knext builder execute \
  --pipeline ./pipelines/builder.yaml \
  --inputs ./data/*.txt
This automatically generates and stores entity & relation embeddings using the configured vectorizer.â€‹

You can inspect ingestion logs in:

text
logs/builder.log
âš™ï¸ 5. Run Inference and Reasoning
After data ingestion, you can run reasoning logic or Q&A queries:

bash
knext reasoner execute \
  --config ./configs/reason.yaml \
  --query "How is entity A related to concept B?"
You can also access the reasoning web client through:

text
http://127.0.0.1:8887/reasoner
ğŸ§¾ 6. Optional: Integrate LLM Reasoning (KGâ€¯+â€¯LLM Fusion)
In the config file, add a section linking an LLM model for fusion logic:

text
reason_llm:
  api_key: sk-<your-openai-key>
  base_url: https://api.openai.com/v1
  model: gpt-5
  type: maas
Then run:

bash
knext thinker execute --query "Summarize insights about topic X using the KB."
This uses neuralâ€‘symbolic fusion reasoning, combining graph logic with LLM generation.â€‹

âœ… 7. Verify the Setup
After completing the above:

Open http://127.0.0.1:8887 â€” UI ready.

Run knext --help â€” CLI recognized.

Confirm vector embeddings exist under /data/vector_store.

Test a reasoning query â€” if it returns text, OpenSPG and KAG are functioning.

ğŸ§­ In Summary
Step	Action	Outcome
1	Start Docker containers	OpenSPG coreâ€¯server active
2	Install KAG toolkit	Commandâ€‘line interface available
3	Createâ€¯KBâ€¯config	Vectorizerâ€¯+â€¯modelâ€¯linked
4	Importâ€¯data	Documents storedâ€¯+â€¯embedded
5	Runâ€¯reasoner	Logical/semanticâ€¯answers
6	(Optional) LLMâ€¯fusion	KBâ€¯+â€¯LLMâ€¯reasoning
7	Verifyâ€¯UIâ€¯&â€¯logs	Fullâ€¯setupâ€¯ready
Following these steps will bring your local OpenSPGâ€¯(KAGâ€¯0.8) environment fully online for document ingestion, knowledge graph construction, and reasoning in 2025.â€‹