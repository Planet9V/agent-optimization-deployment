# Tier 2 Entity Type Validation - Summary Report

**File**: Tier2_Type_Review_Summary.md
**Created**: 2025-11-25
**Review ID**: tier2-type-validation-001
**Status**: READY FOR EXECUTION (Waiting for Tier 1 completion)
**Tier**: 2 - Entity Type Classification Validation

---

## Executive Summary

Tier 2 Entity Type Validation is a comprehensive quality review framework designed to validate the classification accuracy of 18 entity types across the NER10 cybersecurity corpus. This review validates that entities are correctly classified according to the annotation schema and guidelines.

**Mission**: Ensure entity type classifications achieve F1 > 0.80 per type through systematic validation and correction.

**Current Status**: Framework defined and ready for execution. **Blocked by**: Tier 1 boundary review completion.

---

## Prerequisite Status

### ðŸš¨ BLOCKING ISSUE: Tier 1 Not Started

**Required Before Tier 2**:
- âœ… Annotation workflow documented
- âœ… Entity schema complete (18 types)
- âœ… Annotation guidelines complete
- âŒ **Tier 1 boundary review NOT STARTED**
- âŒ **No annotated files available yet**

**Project Phase**: Week 1 planning phase. Annotation begins Week 2-3.

**Expected Availability**: Tier 1 completion expected Week 3-4 after annotation infrastructure is operational.

---

## Scope & Objectives

### Validation Scope

- **Total Files**: 678 markdown documents
- **Sample Size**: 169 files (25% stratified random sample)
- **Entity Types**: 18 (10 technical + 8 psychological)
- **Estimated Entities**: ~2,535 entities (15 per file average)
- **Validation Focus**: Type classification accuracy + attribute correctness

### Key Objectives

1. **Type Classification Validation**: Verify each entity is assigned the correct type from 18-type taxonomy
2. **Attribute Accuracy**: Validate critical attributes (bias_type, perception_type, motivation_type)
3. **Confusion Pattern Detection**: Identify systematic misclassification patterns
4. **F1 Score Achievement**: Ensure 16+ entity types achieve F1 > 0.80
5. **Guideline Improvement**: Document recommendations for annotation guideline enhancements

---

## Entity Type Taxonomy (18 Types)

### Technical Entities (10)

| Entity Type | Primary Validation Focus |
|-------------|-------------------------|
| `EQUIPMENT` | Distinguish from FACILITY; ICS-specific vs general IT |
| `CVE` | Format validation (CVE-YYYY-NNNNN) |
| `SECTOR` | Critical infrastructure sector classification |
| `THREAT_ACTOR` | Distinguish from ORGANIZATION |
| `TECHNIQUE` | MITRE ATT&CK mapping; distinguish from PROCESS |
| `ORGANIZATION` | Legitimate entities only |
| `FACILITY` | Physical locations; distinguish from EQUIPMENT |
| `PROCESS` | Industrial workflows; distinguish from TECHNIQUE |
| `MEASUREMENT` | Quantitative values with units |
| `PROPERTY` | System characteristics |

### Psychological Entities (8)

| Entity Type | Primary Validation Focus |
|-------------|-------------------------|
| `COGNITIVE_BIAS` | **30 bias subtypes** - most complex type to validate |
| `EMOTION` | Distinguish from DEFENSE_MECHANISM |
| `THREAT_PERCEPTION` | **3 subtypes**: Real/Imaginary/Symbolic |
| `ATTACKER_MOTIVATION` | **MICE framework** + 4 additional motives |
| `DEFENSE_MECHANISM` | Distinguish from EMOTION |
| `SECURITY_CULTURE` | Organizational security mindset |
| `HISTORICAL_PATTERN` | Past incident patterns |
| `FUTURE_THREAT` | Emerging threats |

---

## Validation Protocol (5 Phases)

### Phase 1: Random Sampling
- **Action**: Stratified random sample of 169 files (25%)
- **Stratification**: By document category, length, entity density
- **Tool**: sklearn.model_selection.train_test_split
- **Output**: Sample file list with metadata

### Phase 2: Entity Extraction
- **Action**: Extract all entities from sampled files
- **Input**: Tier1-corrected annotations (JSONL or Prodigy DB)
- **Target**: ~2,535 entities
- **Output**: Entity validation dataset with context

### Phase 3: Type Classification Review
- **Action**: Expert reviewer validates type assignments
- **Method**: Manual review with schema guidelines reference
- **Validation Checklist**: Type-specific validation questions
- **Output**: Type validation decisions + correction log

### Phase 4: Research Pattern Application
- **Action**: Apply psychometric research patterns from Phase 1
- **Checks**:
  - Cognitive bias co-occurrence patterns
  - Emotion â†’ Defense mechanism sequences
  - Threat perception â†” Cognitive bias correlations
  - Attacker motivation â†” Technique alignment
- **Output**: Research-informed validation

### Phase 5: Automated Validation
- **Action**: Run automated validation scripts
- **Checks**:
  - Attribute completeness (CRITICAL)
  - CVE format validation (regex)
  - Bias type vocabulary check (30 types)
  - Perception type vocabulary (3 types)
  - Entity distribution anomalies
- **Output**: Automated validation report

---

## Common Type Confusions

### Technical Entity Confusions

| Confusion | Example | Resolution Rule |
|-----------|---------|----------------|
| **EQUIPMENT vs FACILITY** | "Nuclear power plant control system" | FACILITY = location; EQUIPMENT = device |
| **TECHNIQUE vs PROCESS** | "Lateral movement" vs "Batch production" | TECHNIQUE = attack; PROCESS = industrial workflow |
| **ORGANIZATION vs THREAT_ACTOR** | "DHS CISA" vs "APT28" | ORGANIZATION = legitimate; THREAT_ACTOR = malicious |

### Psychological Entity Confusions

| Confusion | Example | Resolution Rule |
|-----------|---------|----------------|
| **EMOTION vs DEFENSE_MECHANISM** | "Anxiety" vs "Denial" | EMOTION = feeling; DEFENSE_MECHANISM = coping response |
| **COGNITIVE_BIAS vs DEFENSE_MECHANISM** | "Confirmation bias" vs "Rationalization" | COGNITIVE_BIAS = thinking error; DEFENSE_MECHANISM = protection |

---

## Critical Validation Checks

### COGNITIVE_BIAS (Most Complex)
- âœ… Is the bias one of **30 defined bias types**?
- âœ… Is `bias_type` attribute correctly assigned?
- âœ… Is `bias_category` correct (7 categories)?
- âœ… Does context support the classification?

**30 Bias Types**: normalcy_bias, confirmation_bias, availability_heuristic, recency_bias, anchoring_bias, framing_effect, overconfidence_bias, optimism_bias, status_quo_bias, sunk_cost_fallacy, groupthink, authority_bias, bandwagon_effect, in_group_bias, halo_effect, present_bias, planning_fallacy, hindsight_bias, temporal_discounting, fundamental_attribution_error, self_serving_bias, actor_observer_bias, selective_attention, inattentional_blindness, change_blindness, complexity_bias, dunning_kruger_effect, illusory_correlation, clustering_illusion, gambler_fallacy

### THREAT_PERCEPTION
- âœ… Is `perception_type` correct: **Real | Imaginary | Symbolic**?
- Real = actual verified threat with evidence
- Imaginary = perceived threat without evidence
- Symbolic = threat to identity/values, not physical
- âœ… Is `evidence_level` attribute appropriate?

### ATTACKER_MOTIVATION
- âœ… Does motivation fit **MICE framework** (Money/Ideology/Coercion/Ego)?
- âœ… Or additional motives: Hacktivism/Espionage/Sabotage/Insider?
- âœ… Is `motivation_type` attribute correct?

### CVE
- âœ… Matches format: `CVE-\d{4}-\d{4,7}`
- âœ… Full identifier captured (not partial)

---

## Quality Metrics

### Type Classification F1 Targets

| Entity Type | Target F1 | Challenge Level |
|-------------|-----------|----------------|
| COGNITIVE_BIAS | 0.75 | **HIGH** (30 subtypes) |
| THREAT_PERCEPTION | 0.80 | **MEDIUM** (context-dependent) |
| EMOTION | 0.80 | **MEDIUM** (overlap with defense) |
| ATTACKER_MOTIVATION | 0.85 | **LOW** (clear framework) |
| EQUIPMENT | 0.85 | **MEDIUM** (ICS-specific) |
| CVE | 0.95 | **LOW** (format-based) |
| TECHNIQUE | 0.80 | **MEDIUM** (MITRE mapping) |
| THREAT_ACTOR | 0.85 | **LOW** (clear distinction) |

### Success Criteria

**PRIMARY**: 16+ entity types achieve F1 > 0.80
**SECONDARY**: Overall type classification accuracy > 0.85
**TERTIARY**: Attribute accuracy > 0.90 for critical attributes

**Pass Gate**: All three criteria met â†’ Proceed to Tier 3 (Relationship Validation)

---

## Confusion Matrix Analysis

### Structure
- **18 x 18 matrix**: Rows = predicted types, Columns = actual types
- **Diagonal**: Correct classifications
- **Off-diagonal**: Misclassifications

### Metrics Calculated
- Precision per entity type
- Recall per entity type
- F1 score per entity type
- Overall classification accuracy

### Visualization
- Heatmap visualization (seaborn)
- Export as `Tier2_Confusion_Matrix.png`

---

## Correction Protocol

### Reclassification Process

1. **Document** original classification and rationale
2. **Identify** correct entity type based on guidelines
3. **Update** entity label in annotation data
4. **Update** attributes if applicable
5. **Log** correction with justification
6. **Flag** for re-annotation if systematic pattern detected

### Escalation Criteria

- **File-level**: >20% entities require reclassification â†’ Re-annotate entire file
- **Type-level**: >30% of specific type misclassified â†’ Review guidelines + re-train
- **Confusion-level**: >15% systematic confusion between two types â†’ Add disambiguation examples

---

## Estimated Corrections

Based on typical annotation quality:

- **Total entities reviewed**: 2,535
- **Estimated corrections**: 380 (15% correction rate)
- **Breakdown**:
  - Type reclassifications: 200 (8%)
  - Attribute corrections: 150 (6%)
  - Boundary-dependent flags: 30 (1%)

---

## Deliverables

### 1. Tier2_Corrected_Annotations.jsonl
**Description**: Annotations with corrected entity types and attributes
**Format**: Prodigy JSONL with updated labels and attributes
**Size**: ~169 files, ~2,535 entities

### 2. Tier2_Type_Validation_Report.json
**Description**: Comprehensive type classification analysis
**Sections**:
- Type F1 scores per entity (18 types)
- Confusion matrix (18x18)
- Common misclassification patterns
- Attribute accuracy metrics
- Correction statistics
- Guidelines improvement recommendations

### 3. Tier2_Confusion_Matrix.png
**Description**: Visual heatmap of type classifications
**Format**: PNG image (18x18 heatmap)
**Tool**: seaborn.heatmap

### 4. Tier2_Correction_Log.csv
**Description**: Detailed log of all type corrections
**Columns**: entity_id, file_source, entity_text, original_type, corrected_type, rationale, reviewer, timestamp

### 5. Tier2_Guideline_Recommendations.md
**Description**: Recommended updates to annotation guidelines
**Content**: Based on common misclassifications and confusion patterns identified

---

## Execution Workflow (8 Steps)

### Step 1: Load Tier 1 Data
- **Input**: Tier1_Boundary_Corrections.json
- **Action**: Load corrected annotations with proper boundaries
- **Output**: DataFrame with entity data

### Step 2: Random Sampling
- **Action**: Stratified random sample 169 files
- **Stratification**: Category, length, entity density
- **Output**: Sample file list

### Step 3: Entity Extraction
- **Action**: Extract all entities from sampled files
- **Target**: ~2,535 entities
- **Output**: Entity validation dataset

### Step 4: Manual Type Review
- **Action**: Expert reviewer validates types
- **Method**: Validation interface with guidelines
- **Output**: Type validation decisions

### Step 5: Automated Checks
- **Action**: Run validation scripts
- **Checks**: Attributes, formats, distributions
- **Output**: Automated validation report

### Step 6: Confusion Matrix
- **Action**: Calculate 18x18 confusion matrix
- **Tool**: sklearn.metrics.confusion_matrix
- **Output**: F1 scores per type

### Step 7: Apply Corrections
- **Action**: Update annotations with corrections
- **Method**: Batch update in Prodigy DB or JSONL
- **Output**: Tier2_Corrected_Annotations.jsonl

### Step 8: Validation Report
- **Action**: Generate comprehensive report
- **Output**: Tier2_Type_Validation_Report.json

---

## Timeline

### Prerequisite
**Tier 1 Completion**: Expected Week 3-4

### Estimated Duration
**5-7 days** after Tier 1 completion

### Daily Breakdown
- **Day 1**: Load data, sampling, entity extraction
- **Day 2-3**: Manual type review (~1,200 entities/day)
- **Day 4**: Automated validation, confusion matrix
- **Day 5**: Apply corrections, generate reports
- **Day 6-7**: Quality review, guideline recommendations

---

## Resource Requirements

### Personnel
- **Expert Reviewer**: 1 person with NLP and psychology expertise
- **Data Engineer**: 1 person for automation scripts

### Compute
- Standard workstation (16GB RAM)
- Python 3.11+ environment
- Libraries: pandas, sklearn, spacy, seaborn

### Tools
- Python 3.11+
- pandas, numpy
- scikit-learn (confusion_matrix, metrics)
- spaCy 3.7+
- Jupyter notebooks
- Prodigy (annotation viewing)

---

## Automation Scripts

### tier2_type_validation.py
**Purpose**: End-to-end Tier 2 validation automation
**Functions**:
- load_tier1_annotations()
- stratified_sample_files(n=169)
- extract_entities()
- validate_attributes()
- calculate_confusion_matrix()
- generate_f1_scores()
- export_corrections()

### validate_attributes.py
**Purpose**: Automated attribute validation
**Checks**:
- COGNITIVE_BIAS has valid bias_type (30 values)
- THREAT_PERCEPTION has valid perception_type (3 values)
- ATTACKER_MOTIVATION has valid motivation_type (8 values)
- CVE matches regex pattern

### generate_confusion_matrix.py
**Purpose**: Confusion matrix calculation and visualization
**Output**: 18x18 matrix with precision, recall, F1 per type

---

## Next Steps After Tier 2

### Tier 3: Relationship Validation
- **Focus**: Validate relationship types between entities
- **Prerequisite**: Tier 2 completion with F1 > 0.80
- **Scope**: 24 relationship types

### Guideline Revision
- **Action**: Update annotation guidelines based on Tier 2 findings
- **Goal**: Reduce confusion patterns identified in review

### Model Training
- **Action**: Use corrected annotations for spaCy model training
- **Goal**: Achieve >0.80 F1 in production model

---

## Risk Assessment

### Anticipated Issues

| Issue | Frequency | Impact | Mitigation |
|-------|-----------|--------|------------|
| COGNITIVE_BIAS subtype confusion | MEDIUM | HIGH | Bias decision tree in guidelines |
| EQUIPMENT vs FACILITY ambiguity | MEDIUM | MEDIUM | More examples |
| EMOTION vs DEFENSE_MECHANISM overlap | LOW | MEDIUM | Temporal indicators |
| THREAT_PERCEPTION misclassification | MEDIUM | HIGH | Evidence-based rubric |

---

## Completion Criteria

### Tier 2 Complete When:
- âœ… 25% of files (169) reviewed
- âœ… ~2,535 entities type-validated
- âœ… Confusion matrix calculated for 18 types
- âœ… Type F1 > 0.80 for 16+ types OR corrective action plan documented
- âœ… Tier2_Corrected_Annotations.jsonl generated
- âœ… Tier2_Type_Validation_Report.json delivered

### Pass Criteria:
- ðŸŸ¢ **Green Light**: F1 > 0.80 for 16+ types â†’ Proceed to Tier 3
- ðŸŸ¡ **Yellow Light**: F1 0.75-0.80 for 13-15 types â†’ Targeted improvements + re-annotate
- ðŸ”´ **Red Light**: F1 < 0.75 for >5 types â†’ Major revision + re-annotate all

---

## Summary

Tier 2 Entity Type Validation is a critical quality gate ensuring entity type classifications meet production standards before proceeding to relationship validation and model training. The validation framework is **comprehensive, automated where possible, and designed to identify systematic issues** for guideline improvement.

**Current Status**: Ready for execution, awaiting Tier 1 completion (Week 3-4).

**Expected Outcome**: High-quality type-corrected annotations with F1 > 0.80 per type, enabling successful spaCy model training.

---

**File**: /home/jim/2_OXOT_Projects_Dev/1_AEON_Cyber_DTv3_2025-11-19/15_NER10_Approach/reports/Tier2_Type_Review_Summary.md
**Related Files**:
- Tier2_Type_Review.json (detailed validation framework)
- 03_ANNOTATION_WORKFLOW_v1.0.md (annotation guidelines)
- 04_NER10_MODEL_ARCHITECTURE_v1.0.md (model architecture)
