
Architecting High-Precision Data Ingestion for Cyber Threat-Risk Digital Twins: A Neo4j-Centric Local Model Strategy


I. Foundational Architecture: The High-Fidelity Knowledge Graph Pipeline

The construction of a complex cybersecurity threat-risk digital twin for critical sector facilities—ranging from fine-grained Software Bill of Materials (SBOM) data to full facility models of factories or water treatment plants—requires an architectural pipeline that prioritizes precision and fidelity above all else.1 Given the mandate to utilize an existing, highly intricate Neo4j schema featuring relationships that can span 20 or more hops and involve up to 12 distinct multi-domain tags, conventional Large Language Model (LLM) extraction techniques are insufficient on their own.

1.1. The Digital Twin and Complex Schema Mandate

The complexity inherent in the defined schema, characterized by deep relational paths (20+ hops), is not an operational burden but a fundamental requirement for accurate risk modeling. This detailed topology is necessary to enable the subsequent Graph Data Science (GDS) functions to accurately trace threat propagation, predict systemic failures, and conduct contextual analysis.1 To support this, the data ingestion must move beyond simple subject-predicate-object extraction to perform contextual multi-hop inference, identifying entities and relationships that are implicitly linked across long text passages.2 The successful extraction of relationships that are multiple steps removed from the initial entity is an essential technical prerequisite for deriving the systemic risk value of an asset.3
The architectural challenge centers on fulfilling the conflicting demands of high accuracy (F1 score) and computational efficiency (local deployment). Achieving domain-specific precision requires extensive customization. Therefore, a strategic decision is made to adopt a hybrid modeling approach, utilizing specialized Small Language Models (SLMs) for rapid, discrete classification tasks, while reserving Parameter-Efficient Fine-Tuning (PEFT) LLMs for the computationally heavier, deep contextual Relationship Extraction (RE).4 This resource allocation methodology mitigates the high operational costs and latency associated with running massive, generalized models for repetitive production tasks.5

1.2. Defining the Dual-Stream Ingestion Architecture

The data pipeline must be architected to process two fundamentally different classes of input data, requiring distinct handling mechanisms prior to unified knowledge graph population.
The first class, Stream A (Unstructured/Semi-structured Sources), includes academic papers, proprietary reports, web pages, and annual reports. These sources contain valuable, unstructured narrative context but demand aggressive preprocessing and sophisticated Natural Language Processing (NLP) techniques for structural extraction. The extraction process is highly susceptible to contextual loss if parsing is not handled with extreme care.
The second class, Stream B (Structured API Feeds), involves deterministic, high-volume inputs such as the NVD API, Vulncheck API, and various specialized threat feeds, including EPSS enrichment data. While these sources provide structured JSON data, they still require specific mapping logic and internal NLP processing to resolve entity descriptions into granular KG nodes and relationships.6
To manage the inherent complexity of integrating these disparate data streams, multiple models, and multi-step validation processes, the pipeline orchestration relies on established GenAI frameworks. Frameworks such as LangChain or LlamaIndex are mandatory for seamless integration with Neo4j and supporting services, including vector search databases, enabling the smooth flow from source parsing, through fine-tuned model inference, to the final confidence-based triple ingestion.7

II. Preprocessing and Heterogeneous Source Normalization

High-fidelity knowledge graph population is predicated on the integrity of the source data. Errors introduced during the initial data normalization stage cannot be corrected by even the most sophisticated downstream models. Therefore, the architecture focuses heavily on mitigating input quality degradation for heterogeneous sources.

2.1. High-Fidelity Document Parsing (Stream A Preprocessing)

Technical documents, particularly PDFs from academic and governmental sources, embed critical context within non-standard layouts, such as tables, captions, and sidebars. Traditional text extraction tools frequently strip this structural metadata, resulting in text blocks that lack essential connections (e.g., a version number associated with a specific component defined in a table).9 If structural context is lost during parsing, the primary cause of low precision in knowledge extraction begins. It becomes impossible for the Relation Extraction model to correctly infer complex relationships if the necessary contextual attributes are not present in the tokenized input.
To counter this, the ingestion architecture must employ specialized, high-accuracy document processors. Tools benchmarked for superior structure preservation, such as Docling, which has demonstrated $97.9\%$ table cell accuracy, are preferred over purely speed-focused alternatives.9 Alternatives like PyMuPDF or PDFPlumber may also serve, but the primary goal is ensuring the parser outputs a standardized, tokenized text format that explicitly retains metadata detailing structural origins (e.g., noting that a sentence was derived from a specific row in a table).10 This fidelity of input is crucial for maintaining the F1 score objectives.

2.2. Structured API Integration and Triplification (Stream B)

The ingestion of structured vulnerability intelligence, such as NVD and Vulncheck feeds, requires programmatic mapping coupled with targeted NLP to maximize schema adherence.

API Data Processing and Enrichment

NVD feeds are delivered in JSON format, providing data such as CVE IDs, descriptions, and CPE match criteria.11 While structured, the free-form description field still contains critical entities (e.g., affected functions, specific exploit methods) that must be identified via Named Entity Recognition (NER) and Relation Extraction (RE) to map granular details to the complex KG schema.6
Crucially, vulnerability prioritization must integrate dynamic threat likelihood data. The EPSS model provides a daily, continuously refreshed estimate of the probability of exploitation activity over the subsequent 30 days.12 This EPSS score is ingested directly as a key, time-series property (:HAS_EPSS_SCORE) on the corresponding vulnerability node. This strategic integration serves as a fundamental causal driver for dynamic risk modeling, moving the digital twin's prioritization capabilities beyond static CVSS scores to provide actionable, real-time threat intelligence.13

Neo4j Ingestion Strategy

For high-volume, continuous API ingestion, data efficiency and integrity are maintained by utilizing Neo4j's transactional APIs and Cypher's MERGE clause.14 This approach ensures that unique entity nodes—keyed by identifiers like CVE-ID, CPE strings, or malicious IP addresses 15—are created only once, preventing the creation of duplicate nodes and relationships.
Table 1 outlines the required transformation from input source to the standardized, high-fidelity triple structure mandated by the Neo4j schema.
Table 1: Input Source to KG Triple Mapping

Input Source Type
Preprocessing Tool
Example Entity/Data
Standardized Triple Format (Neo4j)
Associated Domains
Academic Paper (PDF)
Docling/LlamaParse 9
"APOC 5.3.1 fixed path traversal" [16]
(APOC:5.3.1)--(CVE:X)
Software, Vulnerability, Patch
NVD API (JSON)
Custom Python Script
epss_score: 0.95 12
(CVE:X)--(Score:0.95)
Threat, Risk
Cybersecurity Report (Text)
Fine-Tuned SLM (NER)
"The threat group APT-28 used malware X" [17]
(APT_Group:APT-28)--(Malware:X)
Threat_Actor, Malware, Attack_Type


2.3. Training Data Construction and Schema Alignment

The successful training of specialized local models hinges on the quality and structure of the training corpus. The models must be trained on manually annotated, domain-specific data formatted as $\langle head\_entity, relationship, tail\_entity \rangle$ triples, directly mirroring the required structure for knowledge graph population.3
Initial corpus development should leverage existing foundational cybersecurity datasets, which are useful for identifying generic entities like Malware, Threat Actors, and attack characteristics.15 However, these must be extensively customized and re-annotated. The resulting dataset must explicitly align with the 12 multi-domain tagging classifications and the highly granular relationships (including hierarchical structures) necessary to support the 20+ hop reasoning required by the specific internal schema. This focused annotation effort is the single most important factor for achieving domain specificity and high extraction precision.

III. Training and Fine-Tuning Local Models for Extraction Precision

The architectural solution for achieving localized high accuracy utilizes a bifurcated model strategy, where each model tier is specialized for its designated task, resolving the inherent conflict between model size, inference speed, and domain-specific quality.

3.1. Model Selection and Rationale for the Hybrid Strategy

The pipeline employs a hybrid model structure that leverages the distinct strengths of SLMs and PEFT LLMs:
Tier 1: High-Precision Classification (SLM): BERT or RoBERTa variants remain highly competitive, often defining the state-of-the-art for high F1 score, discrete classification, and Named Entity Recognition (NER).4 A domain-specific SLM, such such as an adaptation of Cyber-BERT or CyNER, should be fine-tuned to handle the 12 required multi-domain tagging classifications and high-speed entity identification.19
Tier 2: Complex Relational Reasoning (PEFT LLM): For the challenging Relationship Extraction (RE) task, particularly those requiring deep contextual understanding and multi-hop inference, a larger, open-source base model (e.g., Mistral or Llama in the 7B–13B range) is necessary.20 To maintain localization and minimize computational overhead, Parameter-Efficient Fine-Tuning (PEFT) methods, particularly QLoRA, are utilized to adjust specific model weights for domain specialization without retraining the entire large model.20
This architectural choice, separating high-speed classification from complex reasoning, acts as an accelerator and precision booster, improving the overall efficiency and robustness of the pipeline compared to relying on a single, generalized LLM for all tasks.5

3.2. Instruction Tuning for Strict Schema Adherence

General-purpose LLMs excel at fluency but cannot guarantee programmatic adherence to a complex, multi-layered knowledge graph schema. The LLM must be transformed into a reliable structural extractor through rigorous instruction tuning.
This methodology, often implemented via the Relation–Head–Facts (RHF) pipeline, involves instruction fine-tuning to force the model to internalize the domain-specific relational regularities defined by the schema.21 The training process explicitly guides the model to produce output in a rigorous format that directly maps to the Cypher triples (Subject, Predicate, Object) required for Neo4j ingestion.18
Despite meticulous instruction tuning, generative models can still fail to produce clean, valid structured output, often surrounding JSON or XML with verbose commentary or extraneous tokens.24 Consequently, an LLM Output Parser is a mandatory post-processing tool. This lightweight package is integrated immediately after the LLM to robustly extract, clean, and validate the desired JSON/XML structures, ensuring that the model is treated as a specialized structured API call rather than a creative engine.24

3.3. Training Methodology for Deep Relationship (20+ Hop) Extraction

The requirement for 20+ hop relationship fidelity demands that the model acquire advanced multi-hop reasoning capabilities. This necessity moves the task beyond simple binary relation extraction, requiring a depth of contextual understanding similar to complex Question Answering (QA) tasks.3
Analysis of multi-hop reasoning failures often points to the breakdown occurring during the relation attribute extraction stage.2 To address this challenge within the constrained resources of a local deployment, architectural enhancements at the transformer level are deployed. Novel mechanisms, such as Back Attention, enable lower layers of the transformer to leverage hidden states from higher layers during attention computation. This enhancement can significantly boost the model’s ability to maintain and process context over long inference chains, potentially allowing a smaller, localized model (e.g., a 1-layer transformer) to achieve the complex contextual performance typically associated with a much deeper architecture.2 Training data must be constructed to explicitly reflect these long-chain dependencies and hierarchical relationships, guiding the model toward the specific graph topology.26

3.4. Multi-Domain Tagging (12 Domain Classifier)

The task of multi-domain tagging across 12 distinct classification domains (e.g., separating entities into categories such as Asset Class, Threat Actor, Vulnerability Type, and Environment Type) is handled by the high-precision Tier 1 SLM.
Empirical studies show that for multi-label, multi-domain classification tasks, the performance of individual binary classifiers or models trained on combined multi-domain datasets often outperforms a single, complex joint multi-label classifier.27 The fine-tuned SLM executes token classification, assigning one or more of the 12 required tags to each identified entity (e.g., a node representing a critical server may simultaneously receive the labels IT_Asset, High_Risk, and Production_Environment). This multi-label capability is essential for defining the attributes of the digital twin nodes, which in turn informs the subsequent GDS risk calculations.

IV. Advanced Extraction and Confidence-Based Reranking Framework

To guarantee the required high precision and structural fidelity to the complex Neo4j schema, a post-extraction validation architecture—the Confidence-Based Reranking Framework—is essential. This framework addresses the fundamental limitations of LLM-based extraction, which may generate facts that are semantically plausible but factually inconsistent or structurally misaligned with the ontology.

4.1. The Critical Role of Reranking

Reranking serves as a sophisticated two-stage approach. The initial LLM pipeline retrieves a broad set of potentially relevant entities and candidate triples (maximizing recall). The subsequent reranking stage applies an advanced, learned ranking technique to identify and prioritize only the most relevant, contextually correct, and schema-compliant content.29
By introducing a dedicated, objective filter, reranking explicitly mitigates the risk of LLM hallucinations or contextual insensitivity, which often leads to inaccurate triples based on simple semantic similarity.29 This measure is the single most effective architectural intervention for converting high-recall extraction outputs into high-precision ingestion into the knowledge graph. The reranking step functions as the objective fidelity metric required for mission-critical KG population.

4.2. Developing the Discriminative Triple Classifier

The core of the reranking framework is a dedicated, high-speed discriminative classifier. This model must be separate from the large generative LLM to maintain speed and modularity. A small-scale transformer model, such as a fine-tuned BERT variant, is specialized for the binary task of triple classification.30
The robust discrimination between valid and invalid facts requires meticulous training data construction utilizing synthesized negative samples. For every positive, schema-compliant triplet $\tau = \langle h, r, t \rangle$ extracted or manually defined, multiple corrupted or implausible negative triplets are generated. These are formed by randomly replacing the head, tail, or relation components.30 A high negative-to-positive sampling ratio (e.g., 1:10) is typically adopted to ensure the discriminative classifier learns a robust decision boundary, focusing specifically on schema consistency and factual likelihood.30 The output of this classifier is a high-fidelity confidence score, $P(\tau)$, representing the probability that the triple adheres to the defined ontology.

4.3. Integrating Confidence Scoring and Canonicalization

The final validity assessment of a potential knowledge fact is determined by the output of the Reranking classifier. While initial entity recognition and relation extraction may produce confidence metrics 32, the Reranker's $P(\tau)$ score is the definitive threshold gate.
A dedicated canonicalization module must be implemented prior to reranking. This module maps all extracted entity variants (e.g., misspellings or aliases generated by the LLM) to a single, standardized name defined within the Neo4j ontology.32 This ensures that the downstream classifier is not required to validate semantic similarity, but only structural and factual compliance, further reinforcing consistency across the KG. Only triples that achieve a validation score exceeding the predetermined mission-critical threshold (e.g., $P \ge 0.95$) are passed forward for final ingestion.

4.4. Architectural Integration of the Reranking Service

To preserve pipeline modularity and throughput, the Reranking Classifier must be deployed as an independent, high-availability service, decoupled from the LLM extraction component.35
The overall Neo4j data ingestion pipeline is defined by the following multi-step flow. The LLM extracts candidate triples, which are immediately canonicalized. The pipeline then submits these candidates to the external Reranking Service. Only validated, high-confidence triples that pass the score threshold are subsequently integrated into the Neo4j Graph. This mechanism allows for continuous iteration and improvement of the discriminative model without affecting the operational status of the upstream extraction models. This decoupling also provides necessary robustness; if the generative LLM introduces a novel, slightly noisy triplet, the highly specialized reranker can still validate its factual structure without requiring resource-intensive retraining of the LLM.
Table 2: The Multi-Step Validation Pipeline for High-Precision Ingestion

Pipeline Stage
Model/Mechanism
Validation Focus
Confidence Metric
Goal
1. Structural Extraction
Docling/LlamaParse 9
Text Layout and Context Retention
Document Quality Score
Ensure contextual input integrity.
2. Triple Extraction (RE)
PEFT LLM (RHF-Tuned) [23]
Semantic Relevance
Initial Logit Score
Generate candidate $\langle h, r, t \rangle$ triples.
3. Canonicalization
LLM Output Parser 24 + Deduplication Logic
Format Adherence & Entity Uniformity
Canonical Match Score [32]
Standardize all entities to ontology names.
4. Triple Reranking
Discriminative BERT Classifier 30
Schema Consistency & Factual Likelihood (Positive/Negative Samples)
Final Confidence $P(\tau)$
Maximize Precision; filter candidates below $P=0.95$.


V. Operationalizing the Neo4j Digital Twin and GDS Integration

The final success metric for the entire ingestion pipeline is not the F1 score of the extraction models, but the actionability and predictive power of the resulting digital twin model when subjected to Graph Data Science (GDS) analysis.

5.1. Optimized Neo4j Ingestion for Complex KGs

Validated, high-confidence triples are ingested into the Neo4j database using optimized methods designed for handling high volumes of structured data. This requires utilizing high-throughput batching techniques and ensuring transactional integrity during bulk data upload. Crucially, complex relationships spanning 20+ hops require rapid traversal. To support this, comprehensive indexing and unique constraints must be applied to all critical nodes (e.g., :Asset, :CVE, :IoC) to ensure that queries, especially those necessary for GDS algorithm execution, maintain sub-second performance.14

5.2. Leveraging Graph Data Science (GDS) for Threat Propagation

The comprehensive, high-precision topology generated by the ML pipeline is the foundation for advanced cyber risk analysis via the Neo4j Graph Data Science Library.36

Criticality Assessment

To identify nodes whose failure would cause the greatest systemic disruption—a core function of a digital twin—centrality algorithms are applied. Betweenness Centrality measures the importance of an asset based on how often it lies on the shortest path between other assets in the facility model.1 Assets with high Betweenness scores, such as a machine routing material between two critical sections of a factory, pose the highest systemic risk and must be prioritized for protection, irrespective of their static security patch level.1

Threat Visibility and Risk Propagation

Other algorithms, such as PageRank, identify highly connected network assets that might possess excessive privileges, making them prime targets for sophisticated attackers.36 Meanwhile, Label Propagation can cluster potentially compromised user accounts or systems exhibiting similar behavioral patterns or connecting relationships.36 The explicit, complete map of all dependencies defined by the 20+ hop schema is essential here, as GDS requires this accurate topology to calculate how dynamic risk—derived from the integrated EPSS scores—propagates outward from an affected node through the network.37

5.3. The Digital Twin in Action: Enhancing SOAR/SIEM

The primary utility of the resulting holistic system is its ability to enhance existing Security Information and Event Management (SIEM) and Security Orchestration, Automation, and Response (SOAR) systems by infusing them with deep contextual awareness.37
The output of the GDS risk calculation layer provides a prioritized list of assets. Nodes identified as being at high risk, based on their criticality score (centrality) and their proximity to actively tracked threats (EPSS/IoC data), are flagged for immediate action. This allows IT teams to prioritize patching and monitoring efforts effectively.37
Furthermore, the highly accurate KG acts as a sophisticated context engine. LLM-powered agents can query the graph, performing multi-hop reasoning to validate compiled threat intelligence from various sources (RSS, STIX feeds, reports). These agents use patterns of planning and reflection to identify specific Indicators of Compromise (IoC).37 By providing this deep, graph-enabled contextual analysis and recommendations based on knowledge bases like MITRE, the system significantly reduces the volume of false positives that commonly plague traditional, non-graph-aware threat intelligence platforms.37

VI. Conclusions and Recommendations

The objective of building a high-precision, schema-adherent ingestion pipeline for a complex Neo4j threat-risk digital twin is achievable through a multi-faceted, hybrid architectural approach. Direct reliance on a single, generalized LLM for this mission-critical application would introduce unacceptable risks related to schema non-adherence, latency, and resource consumption.
The prescriptive architecture recommends the adoption of a hybrid SLM/PEFT LLM strategy, where:
Preprocessing is dominated by high-fidelity document parsers (e.g., Docling) to ensure structural context is preserved.
Extraction utilizes PEFT-tuned open-source models (Mistral/Llama) specialized via instruction tuning (RHF pipeline) and optimized for deep contextual reasoning through techniques like Back Attention.
Precision Guarantee is established by mandatory, post-extraction validation via a dedicated, discriminative BERT-based Reranking Classifier trained extensively on negative samples. This reranking service acts as the final gate, ensuring only structurally valid facts exceeding a high confidence threshold are ingested into Neo4j.
This sophisticated pipeline design ensures that the data ingested into the Neo4j Graph is not merely relevant, but objectively compliant with the complex, 20+ hop schema. The high fidelity of the resulting digital twin is the essential prerequisite for generating accurate GDS calculations (e.g., Betweenness Centrality) that ultimately deliver the required operational value: precise, contextualized risk prioritization for critical sector facilities.

Future Work

Future development should focus on enhancing the pipeline’s agility and robustness. This includes integrating advanced LLM agents that leverage planning and reflection patterns to systematically validate incoming threat intelligence against the established graph topology before final ingestion.37 Furthermore, implementing continuous adaptation strategies, possibly through active learning, will allow the models to efficiently incorporate emerging threat patterns and new facility components into the training distribution, maintaining high performance as the cybersecurity landscape evolves.
Works cited
Manage Risk with a Digital Twin in Manufacturing Data using Neo4j Graph Analytics, accessed November 1, 2025, https://quickstarts.snowflake.com/guide/managing-risk-in-manufacturing/index.html?index=..%2F..index&utm_cta=drift
[2502.10835] Back Attention: Understanding and Enhancing Multi-Hop Reasoning in Large Language Models - arXiv, accessed November 1, 2025, https://arxiv.org/abs/2502.10835
A Comprehensive Survey on Relation Extraction: Recent Advances and New Frontiers, accessed November 1, 2025, https://arxiv.org/html/2306.02051v3
Are We Really Making Much Progress in Text Classification? A Comparative Review - arXiv, accessed November 1, 2025, https://arxiv.org/html/2204.03954v6
Small Language Models: A Business Leader's Guide to Affordable, Task-Tuned AI, accessed November 1, 2025, https://deliveringdataanalytics.com/small-language-models-business-guide/
Constructing a Knowledge Graph from Textual Descriptions of Software Vulnerabilities in the National Vulnerability Database - ResearchGate, accessed November 1, 2025, https://www.researchgate.net/publication/370442894_Constructing_a_Knowledge_Graph_from_Textual_Descriptions_of_Software_Vulnerabilities_in_the_National_Vulnerability_Database
LangChain Neo4j Integration - Neo4j Labs, accessed November 1, 2025, https://neo4j.com/labs/genai-ecosystem/langchain/
Integrate Qdrant and Neo4j to Enhance Your RAG Pipeline - Graph Database & Analytics, accessed November 1, 2025, https://neo4j.com/blog/developer/qdrant-to-enhance-rag-pipeline/
PDF Data Extraction Benchmark 2025: Comparing Docling, Unstructured, and LlamaParse for Document Processing Pipelines - Procycons, accessed November 1, 2025, https://procycons.com/en/blogs/pdf-data-extraction-benchmark/
A Comparative Study of PDF Parsing Tools Across Diverse Document Categories - arXiv, accessed November 1, 2025, https://arxiv.org/html/2410.09871v1
Vulnerability APIs - NVD, accessed November 1, 2025, https://nvd.nist.gov/developers/vulnerabilities
The EPSS Model - FIRST — Forum of Incident Response and Security Teams, accessed November 1, 2025, https://www.first.org/epss/model
Vulnerability Management Chaining: An Integrated Framework for Efficient Cybersecurity Risk Prioritization - arXiv, accessed November 1, 2025, https://arxiv.org/html/2506.01220v2
How to ingest data effectively in Neo4j - Stack Overflow, accessed November 1, 2025, https://stackoverflow.com/questions/62147346/how-to-ingest-data-effectively-in-neo4j
Improving Data Extraction from Cybersecurity Incident Reports - Software Engineering Institute, accessed November 1, 2025, https://www.sei.cmu.edu/blog/improving-data-extraction-from-cybersecurity-incident-reports/
NLP Based Cyber Security Dataset - Kaggle, accessed November 1, 2025, https://www.kaggle.com/datasets/hussainsheikh03/nlp-based-cyber-security-dataset
Fine-tuning or prompting on LLMs: evaluating knowledge graph construction task - PMC, accessed November 1, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC12237976/
A Novel Approach for Cyber Threat Analysis Systems Using BERT Model from Cyber Threat Intelligence Data - MDPI, accessed November 1, 2025, https://www.mdpi.com/2073-8994/17/4/587
How to Fine-Tune an Open-Source LLM for a Custom Use Case - The 4Geeks Blog, accessed November 1, 2025, https://blog.4geeks.io/how-to-fine-tune-an-open-source-llm-for-a-custom-use-case/
[2508.14427] Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models - arXiv, accessed November 1, 2025, https://arxiv.org/abs/2508.14427
Benchmark various LLM Structured Output frameworks: Instructor, Mirascope, Langchain, LlamaIndex, Fructose, Marvin, Outlines, etc on tasks like multi-label classification, named entity recognition, synthetic data generation, etc. - GitHub, accessed November 1, 2025, https://github.com/stephenleo/llm-structured-output-benchmarks
LLM-empowered knowledge graph construction: A survey - arXiv, accessed November 1, 2025, https://arxiv.org/html/2510.20345v1
Extracting Structured Data from LLM Responses : r/Python - Reddit, accessed November 1, 2025, https://www.reddit.com/r/Python/comments/1jn9nh7/extracting_structured_data_from_llm_responses/
TRANSFORMER-XH: MULTI-EVIDENCE REASONING WITH EXTRA HOP ATTENTION - OpenReview, accessed November 1, 2025, https://openreview.net/pdf?id=r1eIiCNYwS
Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey *This work was supported by JSPS KAKENHI Grant Number JP20H04295. - arXiv, accessed November 1, 2025, https://arxiv.org/html/2402.04854v7
Fine-tuning for multi-domain and multi-label uncivil language detection - ACL Anthology, accessed November 1, 2025, https://aclanthology.org/2020.alw-1.4/
LLM Fine Tuning: The Guide for ML Teams - Label Your Data, accessed November 1, 2025, https://labelyourdata.com/articles/llm-fine-tuning
Reranking in RAG Pipelines: A Complete Guide with Hands-On Implementation, accessed November 1, 2025, https://atalupadhyay.wordpress.com/2025/06/19/reranking-in-rag-pipelines-a-complete-guide-with-hands-on-implementation/
Simple Knowledge Graph Completion Model Based on Differential Negative Sampling and Prompt Learning - MDPI, accessed November 1, 2025, https://www.mdpi.com/2078-2489/14/8/450
arXiv:2402.02389v2 [cs.CL] 23 Feb 2024, accessed November 1, 2025, https://arxiv.org/pdf/2402.02389
[D] Entity Extraction with LLMs : r/MachineLearning - Reddit, accessed November 1, 2025, https://www.reddit.com/r/MachineLearning/comments/1dwbcp5/d_entity_extraction_with_llms/
Complete Custom Named-Entity Recognition (NER) to Relation Extraction (RE) Pipeline, accessed November 1, 2025, https://medium.com/@pasdan/complete-custom-named-entity-recognition-ner-to-relation-extraction-re-pipeline-b9a6db8c90cb
Knowledge Graph Construction: Extraction, Learning, and Evaluation - MDPI, accessed November 1, 2025, https://www.mdpi.com/2076-3417/15/7/3727
HetaRAG: Hybrid Deep Retrieval-Augmented Generation across Heterogeneous Data Stores - arXiv, accessed November 1, 2025, https://arxiv.org/html/2509.21336v1
Graph Analytics Applications and Use Cases - Neo4j, accessed November 1, 2025, https://neo4j.com/blog/aura-graph-analytics/graph-analytics-use-cases/
Cybersecurity Risk Assessment Using LLM Agents and Graph Data Science - Neo4j, accessed November 1, 2025, https://neo4j.com/nodes2024/agenda/cybersecurity-risk-assessment-using-llm-agents-and-graph-data-science/
