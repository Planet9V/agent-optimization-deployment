# API Testing Execution Checklist

## Pre-Execution Checklist

### âœ… Step 1: Container Verification (2 minutes)

```bash
# Check container is running
â–¡ docker ps | grep api-container

# Expected: Container shows as "Up"
# If not running: docker start api-container
```

### âœ… Step 2: Middleware Fix Verification (1 minute)

```bash
# Verify bodyParser middleware is in place
â–¡ grep -A 5 "bodyParser" server.js

# Expected: Should see bodyParser.json() and bodyParser.urlencoded()
# If missing: Apply middleware fix first
```

### âœ… Step 3: API Health Check (1 minute)

```bash
# Test API is responding
â–¡ curl http://localhost:3000/api/health

# Expected: 200 OK response
# If failed: Check server logs with docker logs api-container
```

### âœ… Step 4: Dependencies Installation (1 minute)

```bash
# Navigate to test directory
â–¡ cd tests/api-comprehensive

# Install dependencies
â–¡ npm install

# Expected: Dependencies installed without errors
```

### âœ… Step 5: Qdrant Setup (Optional, 2 minutes)

```bash
# Start Qdrant container
â–¡ docker run -d -p 6333:6333 --name qdrant qdrant/qdrant

# Verify Qdrant is running
â–¡ curl http://localhost:6333/collections

# Expected: JSON response with collections
# If failed: Can skip this step, results will still save to files
```

## Execution Checklist

### ðŸš€ Step 6: Run Complete Test Suite (3-5 minutes)

```bash
# Execute all 232 API tests
â–¡ ./run-tests.sh --all

# Expected output:
# âœ… API is available
# ðŸ“¦ Testing NER APIs...
# ðŸ“¦ Testing SBOM APIs...
# ... (continues for all categories)
# ============================================================
# ðŸ“Š TEST SUMMARY
# ============================================================
# Total Tests: 232
# âœ… Passed: XXX (XX.XX%)
# âŒ Failed: XX (X.XX%)
# âš ï¸ Errors: X (X.XX%)
# â±ï¸ Total Time: XXX.XXs
# ============================================================
```

### ðŸ“Š Step 7: Review Results (5 minutes)

```bash
# View summary report
â–¡ cat results/COMPLETE_API_TEST_RESULTS.md | head -100

# Check for failures
â–¡ jq '.[] | select(.status != "PASS")' results/results-*.json | tail -1 | head -20

# Review performance
â–¡ jq 'sort_by(.response_time) | reverse | .[0:10]' results/results-*.json | tail -1

# Expected:
# - Pass rate > 90%
# - Average response time < 500ms
# - Detailed failure information available
```

### ðŸ’¾ Step 8: Store in Qdrant (2 minutes)

```bash
# Find latest results file
â–¡ LATEST=$(ls -t results/results-*.json | head -1)

# Store in Qdrant
â–¡ npx ts-node qdrant-storage.ts store "$LATEST"

# Get statistics
â–¡ npx ts-node qdrant-storage.ts stats

# Expected:
# âœ… All results stored in Qdrant
# Statistics showing test breakdown
```

### ðŸ“ˆ Step 9: Analysis and Reporting (5 minutes)

```bash
# Generate category breakdown
â–¡ jq -r '.[] | .category' results/results-*.json | tail -1 | sort | uniq -c

# Check failed tests by category
â–¡ jq -r '.[] | select(.status != "PASS") | .category' results/results-*.json | tail -1 | sort | uniq -c

# Get slowest endpoints
â–¡ jq 'sort_by(.response_time) | reverse | .[0:10] | .[] | {endpoint: .endpoint, time: .response_time}' results/results-*.json | tail -1

# Expected: Clear breakdown of results by category and performance
```

## Post-Execution Checklist

### âœ… Step 10: Document Findings (10 minutes)

```bash
# Copy main report to docs
â–¡ cp results/COMPLETE_API_TEST_RESULTS.md ../../docs/API_TEST_RESULTS_$(date +%Y%m%d).md

# Expected: Report saved for reference
```

### âœ… Step 11: Identify Issues (Variable)

```bash
# List all failed tests with details
â–¡ jq '.[] | select(.status != "PASS") | {category, endpoint, status, error}' results/results-*.json | tail -1

# Expected: Clear list of issues to address
```

### âœ… Step 12: Create Action Items (10 minutes)

Based on results, create issues for:

```
â–¡ Fix all 5xx server errors (highest priority)
â–¡ Review 4xx client errors (verify if expected or bug)
â–¡ Optimize slow endpoints (>1000ms response time)
â–¡ Document any expected failures
â–¡ Plan retest after fixes
```

## Success Criteria Verification

### Overall Metrics

```
â–¡ Total APIs tested: 232 âœ“
â–¡ Pass rate: ___% (Target: > 95%)
â–¡ Average response time: ___ms (Target: < 200ms)
â–¡ Server errors (5xx): ___% (Target: 0%)
â–¡ Total test time: ___s (Target: < 300s)
```

### Category Metrics

```
â–¡ NER APIs: ___/5 passed (Target: 100%)
â–¡ SBOM APIs: ___/32 passed (Target: > 95%)
â–¡ Vendor Equipment APIs: ___/28 passed (Target: > 90%)
â–¡ Threat Intel APIs: ___/27 passed (Target: > 95%)
â–¡ Risk Scoring APIs: ___/26 passed (Target: > 95%)
â–¡ Remediation APIs: ___/29 passed (Target: > 90%)
â–¡ Compliance APIs: ___/28 passed (Target: > 95%)
â–¡ Scanning APIs: ___/30 passed (Target: > 90%)
â–¡ Alerts APIs: ___/32 passed (Target: > 95%)
â–¡ Economic APIs: ___/26 passed (Target: > 90%)
â–¡ Demographics APIs: ___/24 passed (Target: > 90%)
â–¡ Prioritization APIs: ___/28 passed (Target: > 95%)
â–¡ Next.js APIs: ___/64 passed (Target: > 95%)
â–¡ OpenSPG APIs: ___/40 passed (Target: > 90%)
```

## Deliverables Checklist

### Required Files

```
â–¡ results/COMPLETE_API_TEST_RESULTS.md (Primary deliverable)
â–¡ results/results-TIMESTAMP.json (Detailed results)
â–¡ results/results-TIMESTAMP.csv (Spreadsheet export)
â–¡ Qdrant storage completed (if using)
```

### Documentation

```
â–¡ Test summary documented
â–¡ Failures documented with details
â–¡ Performance issues identified
â–¡ Recommendations created
â–¡ Next steps defined
```

## Troubleshooting Checklist

If tests fail to run:

```
â–¡ Verify container is running
â–¡ Check API health endpoint
â–¡ Review server logs
â–¡ Verify middleware fix
â–¡ Check network connectivity
â–¡ Verify dependencies installed
```

If high failure rate:

```
â–¡ Test single category to isolate
â–¡ Review server error logs
â–¡ Check middleware configuration
â–¡ Verify API endpoints exist
â–¡ Test manually with curl
```

If Qdrant fails:

```
â–¡ Check Qdrant container running
â–¡ Verify port 6333 accessible
â–¡ Test with curl
â–¡ Restart Qdrant if needed
â–¡ Skip Qdrant and use file results
```

## Timeline Estimate

| Phase | Duration | Cumulative |
|-------|----------|------------|
| Pre-execution checks | 5 min | 5 min |
| Test execution | 3-5 min | 8-10 min |
| Results review | 5 min | 13-15 min |
| Qdrant storage | 2 min | 15-17 min |
| Analysis | 5 min | 20-22 min |
| Documentation | 10 min | 30-32 min |
| Action items | 10 min | 40-42 min |
| **TOTAL** | **40-42 min** | - |

## Final Sign-Off

```
â–¡ All 232 APIs tested
â–¡ Results documented in COMPLETE_API_TEST_RESULTS.md
â–¡ Results stored in Qdrant (if applicable)
â–¡ Failures documented and analyzed
â–¡ Action items created
â–¡ Next steps defined
â–¡ Deliverable ready for review
```

## Notes

- Keep results files for historical comparison
- Store in version control for tracking
- Use Qdrant for long-term trend analysis
- Retest after fixes are applied
- Compare results across test runs

---

**Status**: Ready for execution
**Command**: `./run-tests.sh --all`
**Expected Duration**: 40-42 minutes total
**Primary Deliverable**: `results/COMPLETE_API_TEST_RESULTS.md`
