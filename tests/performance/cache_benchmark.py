#!/usr/bin/env python3
"""
Neo4j Cache Performance Benchmark
Validates 150-12,500x speedup claims with statistical rigor
"""

import time
import statistics
from typing import List, Dict, Tuple
from neo4j import GraphDatabase
import json
import os
from datetime import datetime

class CacheBenchmark:
    def __init__(self, uri: str, user: str, password: str):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "iterations": 100,
            "queries": {}
        }

    def close(self):
        self.driver.close()

    def warmup_cache(self, query: str, params: dict = None):
        """Execute query once to warm up cache"""
        with self.driver.session() as session:
            session.run(query, params or {})

    def measure_latency(self, query: str, params: dict = None, iterations: int = 100) -> List[float]:
        """Measure query latency over multiple iterations (milliseconds)"""
        latencies = []

        for _ in range(iterations):
            start = time.perf_counter()
            with self.driver.session() as session:
                result = session.run(query, params or {})
                _ = list(result)  # Force evaluation
            end = time.perf_counter()
            latencies.append((end - start) * 1000)  # Convert to milliseconds

        return latencies

    def clear_cache(self):
        """Clear Neo4j page cache (requires enterprise or community workaround)"""
        # For community edition, we'll use a large query to flush cache
        flush_query = """
        MATCH (n)
        WITH n LIMIT 100000
        RETURN count(n)
        """
        with self.driver.session() as session:
            session.run(flush_query)

        # Wait for cache to stabilize
        time.sleep(2)

    def calculate_statistics(self, latencies: List[float]) -> Dict[str, float]:
        """Calculate statistical metrics"""
        return {
            "p50_ms": statistics.median(latencies),
            "p95_ms": statistics.quantiles(latencies, n=20)[18],  # 95th percentile
            "p99_ms": statistics.quantiles(latencies, n=100)[98],  # 99th percentile
            "mean_ms": statistics.mean(latencies),
            "stdev_ms": statistics.stdev(latencies),
            "min_ms": min(latencies),
            "max_ms": max(latencies)
        }

    def benchmark_query(self, name: str, query: str, params: dict = None) -> Dict:
        """
        Benchmark a query with cache cold and warm scenarios

        Returns:
            Dict with cache_miss, l2_hit, l1_hit metrics and speedup factors
        """
        print(f"\nüî¨ Benchmarking: {name}")

        # Scenario 1: Cache MISS (cold cache)
        print("  üìä Measuring cache MISS (cold cache)...")
        self.clear_cache()
        cache_miss_latencies = self.measure_latency(query, params, iterations=10)
        cache_miss_stats = self.calculate_statistics(cache_miss_latencies)

        # Scenario 2: L2 Cache HIT (warm cache, first access)
        print("  üìä Measuring L2 cache HIT...")
        self.warmup_cache(query, params)
        time.sleep(1)  # Let cache settle
        l2_hit_latencies = self.measure_latency(query, params, iterations=50)
        l2_hit_stats = self.calculate_statistics(l2_hit_latencies)

        # Scenario 3: L1 Cache HIT (hot cache, repeated access)
        print("  üìä Measuring L1 cache HIT...")
        l1_hit_latencies = self.measure_latency(query, params, iterations=100)
        l1_hit_stats = self.calculate_statistics(l1_hit_latencies)

        # Calculate speedup factors
        speedup_l1 = cache_miss_stats["p50_ms"] / l1_hit_stats["p50_ms"]
        speedup_l2 = cache_miss_stats["p50_ms"] / l2_hit_stats["p50_ms"]

        results = {
            "cache_miss": cache_miss_stats,
            "l2_hit": l2_hit_stats,
            "l1_hit": l1_hit_stats,
            "speedup_l1": round(speedup_l1, 2),
            "speedup_l2": round(speedup_l2, 2),
            "target_l1_ms": 1.0,
            "target_l2_ms": 10.0,
            "meets_l1_target": l1_hit_stats["p50_ms"] < 1.0,
            "meets_l2_target": l2_hit_stats["p50_ms"] < 10.0
        }

        print(f"  ‚úÖ L1 Speedup: {speedup_l1:.1f}x (target: 150-12,500x)")
        print(f"  ‚úÖ L2 Speedup: {speedup_l2:.1f}x")
        print(f"  üìà L1 p50: {l1_hit_stats['p50_ms']:.3f}ms (target: <1ms)")
        print(f"  üìà L2 p50: {l2_hit_stats['p50_ms']:.3f}ms (target: <10ms)")

        return results

    def run_benchmarks(self):
        """Execute all benchmark queries"""

        # Test Query 1: Simple node lookup by ID
        self.results["queries"]["node_lookup_by_id"] = self.benchmark_query(
            name="Node Lookup by ID",
            query="MATCH (n) WHERE id(n) = $id RETURN n",
            params={"id": 0}
        )

        # Test Query 2: Property-based lookup with index
        self.results["queries"]["property_lookup"] = self.benchmark_query(
            name="Property Lookup (Indexed)",
            query="MATCH (c:CriticalInfrastructure {assetId: $assetId}) RETURN c",
            params={"assetId": "ASSET-001"}
        )

        # Test Query 3: Relationship traversal (1-hop)
        self.results["queries"]["relationship_traversal"] = self.benchmark_query(
            name="Relationship Traversal (1-hop)",
            query="""
            MATCH (a:Asset)-[:CONNECTS_TO]->(b:Asset)
            WHERE a.assetId = $assetId
            RETURN b
            """,
            params={"assetId": "ASSET-001"}
        )

        # Test Query 4: Pattern matching (2-hop)
        self.results["queries"]["pattern_matching"] = self.benchmark_query(
            name="Pattern Matching (2-hop)",
            query="""
            MATCH (a:Asset)-[:CONNECTS_TO*1..2]->(b:Asset)
            WHERE a.assetId = $assetId
            RETURN b
            LIMIT 100
            """,
            params={"assetId": "ASSET-001"}
        )

        # Test Query 5: Aggregation query
        self.results["queries"]["aggregation"] = self.benchmark_query(
            name="Aggregation Query",
            query="""
            MATCH (c:CriticalInfrastructure)
            RETURN c.sector, count(c) as count
            """,
            params={}
        )

        # Calculate overall statistics
        self.calculate_overall_stats()

        return self.results

    def calculate_overall_stats(self):
        """Calculate aggregate statistics across all queries"""
        all_l1_speedups = []
        all_l2_speedups = []
        all_l1_latencies = []
        all_l2_latencies = []

        for query_name, stats in self.results["queries"].items():
            all_l1_speedups.append(stats["speedup_l1"])
            all_l2_speedups.append(stats["speedup_l2"])
            all_l1_latencies.append(stats["l1_hit"]["p50_ms"])
            all_l2_latencies.append(stats["l2_hit"]["p50_ms"])

        self.results["overall"] = {
            "avg_l1_speedup": round(statistics.mean(all_l1_speedups), 2),
            "max_l1_speedup": round(max(all_l1_speedups), 2),
            "min_l1_speedup": round(min(all_l1_speedups), 2),
            "avg_l2_speedup": round(statistics.mean(all_l2_speedups), 2),
            "avg_l1_latency_ms": round(statistics.mean(all_l1_latencies), 3),
            "avg_l2_latency_ms": round(statistics.mean(all_l2_latencies), 3),
            "l1_target_achieved": statistics.mean(all_l1_latencies) < 1.0,
            "l2_target_achieved": statistics.mean(all_l2_latencies) < 10.0,
            "claimed_speedup_range": "150-12,500x",
            "validation": self.validate_claims(all_l1_speedups)
        }

    def validate_claims(self, speedups: List[float]) -> str:
        """Validate speedup claims against actual measurements"""
        max_speedup = max(speedups)
        avg_speedup = statistics.mean(speedups)

        if max_speedup >= 150:
            return f"‚úÖ VALIDATED: Max speedup {max_speedup:.1f}x meets minimum claim of 150x"
        elif avg_speedup >= 100:
            return f"‚ö†Ô∏è PARTIAL: Avg speedup {avg_speedup:.1f}x approaches claim, max {max_speedup:.1f}x"
        else:
            return f"‚ùå UNVALIDATED: Max speedup {max_speedup:.1f}x below claimed 150x minimum"

    def save_results(self, filepath: str):
        """Save benchmark results to JSON"""
        with open(filepath, 'w') as f:
            json.dump(self.results, f, indent=2)
        print(f"\nüíæ Results saved to: {filepath}")

    def print_summary(self):
        """Print executive summary of results"""
        print("\n" + "="*80)
        print("üìä PERFORMANCE BENCHMARK SUMMARY")
        print("="*80)

        overall = self.results["overall"]

        print(f"\nüéØ SPEEDUP ANALYSIS:")
        print(f"  Average L1 Speedup: {overall['avg_l1_speedup']}x")
        print(f"  Maximum L1 Speedup: {overall['max_l1_speedup']}x")
        print(f"  Minimum L1 Speedup: {overall['min_l1_speedup']}x")
        print(f"  Average L2 Speedup: {overall['avg_l2_speedup']}x")

        print(f"\n‚è±Ô∏è  LATENCY ANALYSIS:")
        print(f"  Average L1 Hit: {overall['avg_l1_latency_ms']:.3f}ms (target: <1ms)")
        print(f"  Average L2 Hit: {overall['avg_l2_latency_ms']:.3f}ms (target: <10ms)")
        print(f"  L1 Target: {'‚úÖ ACHIEVED' if overall['l1_target_achieved'] else '‚ùå MISSED'}")
        print(f"  L2 Target: {'‚úÖ ACHIEVED' if overall['l2_target_achieved'] else '‚ùå MISSED'}")

        print(f"\nüî¨ VALIDATION:")
        print(f"  Claimed Range: {overall['claimed_speedup_range']}")
        print(f"  {overall['validation']}")

        print("\n" + "="*80)


def main():
    # Neo4j connection configuration
    NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
    NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
    NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "your_password")

    print("üöÄ Neo4j Cache Performance Benchmark")
    print(f"üìç Connecting to: {NEO4J_URI}")

    benchmark = CacheBenchmark(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)

    try:
        # Run all benchmarks
        results = benchmark.run_benchmarks()

        # Save results
        output_file = "/home/jim/2_OXOT_Projects_Dev/tests/performance/benchmark_results.json"
        benchmark.save_results(output_file)

        # Print summary
        benchmark.print_summary()

    finally:
        benchmark.close()


if __name__ == "__main__":
    main()
