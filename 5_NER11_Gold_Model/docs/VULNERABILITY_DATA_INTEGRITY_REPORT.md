# Vulnerability Data Integrity Report

**Date**: 2025-12-02
**Session**: E30 Extended Analysis
**Author**: Claude Code
**Status**: Analysis Complete

---

## Executive Summary

Based on comprehensive investigation, the vulnerability data architecture is **sound but requires targeted improvements**. The apparent "disjointedness" is actually by design - the system correctly separates:
- **CVE Taxonomy** (316,552 structured nodes) - The authoritative vulnerability database
- **Entity Extractions** (49 CVE entities) - CVE mentions found in ingested documents

This is the correct architecture. However, there are gaps that need addressing.

---

## 1. Should You Be Concerned?

### âœ… What's Working Correctly

| Component | Status | Details |
|-----------|--------|---------|
| CVE Taxonomy | âœ… Strong | 316,552 CVEs with EPSS scores (94.9% coverage) |
| CWE Taxonomy | âœ… Strong | 2,177 weaknesses with cross-references |
| CAPEC Taxonomy | âœ… Strong | 613 attack patterns |
| CVEâ†’CWE Links | âœ… Strong | 232,752 HAS_WEAKNESS relationships |
| CWEâ†”CAPEC Links | âœ… Strong | 1,209 bidirectional relationships |
| Entityâ†’CVE Links | âœ… Working | 86 cross-domain links (43 REFERENCES + 43 INSTANCE_OF) |

### âš ï¸ Areas Requiring Attention

| Gap | Severity | Impact | Fix Effort |
|-----|----------|--------|------------|
| **CPE Missing** | ðŸŸ¡ Medium | No software product â†’ CVE mapping | Medium (data load) |
| **CVSS at 0%** | ðŸŸ¡ Medium | No severity scores from NVD | Low (API enrichment) |
| **Low CVE Entity Count** | ðŸŸ¢ Low | Expected - most docs don't contain CVEs | N/A (not a bug) |
| **5 Orphan CVEs** | ðŸŸ¢ Low | Test/synthetic CVE IDs not in taxonomy | Low (cleanup) |

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STRUCTURED TAXONOMY LAYER                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   CVE    â”‚â”€â”€â–¶â”‚   CWE    â”‚â—€â”€â–¶â”‚  CAPEC   â”‚   â”‚   CPE    â”‚     â”‚
â”‚  â”‚ 316,552  â”‚   â”‚  2,177   â”‚   â”‚   613    â”‚   â”‚    âŒ    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ MISSING  â”‚     â”‚
â”‚       â”‚ EPSS: 94.9%                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚       â”‚ CVSS: 0%                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”‚ REFERENCES / INSTANCE_OF (86 links)
        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ENTITY EXTRACTION LAYER                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                  Entity Nodes (216,983)                   â”‚   â”‚
â”‚  â”‚  - CVE: 49 (mentions found in documents)                  â”‚   â”‚
â”‚  â”‚  - ATTACK_TECHNIQUE: 54                                   â”‚   â”‚
â”‚  â”‚  - VULNERABILITY: 15                                      â”‚   â”‚
â”‚  â”‚  - Other security entities: thousands                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. What Happens When We Update With New CVEs?

### Current CVE Update Process

The CVE Taxonomy is **static** - loaded once from NVD/MITRE sources. To update:

```python
# Recommended Update Strategy
1. Download latest NVD JSON feeds
2. Parse new CVEs since last update
3. MERGE into Neo4j (idempotent upsert)
4. Enrich with EPSS scores via API
5. Optionally add CVSS from NVD API
```

### Proposed Incremental Update Script

```python
#!/usr/bin/env python3
"""
CVE Taxonomy Incremental Update Script

Updates CVE taxonomy with new vulnerabilities from NVD.
"""

from neo4j import GraphDatabase
import requests
from datetime import datetime, timedelta

NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "neo4j@openspg"

NVD_API = "https://services.nvd.nist.gov/rest/json/cves/2.0"
EPSS_API = "https://api.first.org/data/v1/epss"

def get_new_cves(days_back=7):
    """Fetch CVEs published in last N days."""
    start_date = (datetime.now() - timedelta(days=days_back)).strftime("%Y-%m-%dT00:00:00.000")
    end_date = datetime.now().strftime("%Y-%m-%dT23:59:59.999")

    params = {
        "pubStartDate": start_date,
        "pubEndDate": end_date
    }

    response = requests.get(NVD_API, params=params, timeout=60)
    if response.status_code == 200:
        return response.json().get("vulnerabilities", [])
    return []

def get_epss_scores(cve_ids):
    """Fetch EPSS scores for CVE list."""
    scores = {}
    # EPSS API accepts comma-separated CVE IDs
    chunk_size = 100
    for i in range(0, len(cve_ids), chunk_size):
        chunk = cve_ids[i:i+chunk_size]
        response = requests.get(f"{EPSS_API}?cve={','.join(chunk)}")
        if response.status_code == 200:
            for item in response.json().get("data", []):
                scores[item["cve"]] = {
                    "epss": float(item["epss"]),
                    "percentile": float(item["percentile"])
                }
    return scores

def update_neo4j(cves, epss_scores, driver):
    """Insert/update CVEs in Neo4j."""
    with driver.session() as session:
        for vuln in cves:
            cve = vuln.get("cve", {})
            cve_id = cve.get("id")

            # Get EPSS if available
            epss = epss_scores.get(cve_id, {})

            # Get CVSS if available
            metrics = cve.get("metrics", {})
            cvss31 = metrics.get("cvssMetricV31", [{}])[0].get("cvssData", {})

            session.run("""
                MERGE (c:CVE {id: $cve_id})
                SET c.description = $description,
                    c.published = $published,
                    c.last_modified = $modified,
                    c.epss_score = $epss,
                    c.epss_percentile = $percentile,
                    c.cvss_score = $cvss,
                    c.cvss_vector = $vector,
                    c.updated_at = timestamp()
            """,
                cve_id=cve_id,
                description=cve.get("descriptions", [{}])[0].get("value", ""),
                published=cve.get("published"),
                modified=cve.get("lastModified"),
                epss=epss.get("epss", 0.0),
                percentile=epss.get("percentile", 0.0),
                cvss=cvss31.get("baseScore", 0.0),
                vector=cvss31.get("vectorString", "")
            )

            # Link to CWEs
            for weakness in cve.get("weaknesses", []):
                for desc in weakness.get("description", []):
                    cwe_id = desc.get("value", "")
                    if cwe_id.startswith("CWE-"):
                        session.run("""
                            MATCH (c:CVE {id: $cve_id})
                            MATCH (w:CWE {id: $cwe_id})
                            MERGE (c)-[r:HAS_WEAKNESS]->(w)
                        """, cve_id=cve_id, cwe_id=cwe_id.lower())

def main():
    print("=== CVE Taxonomy Update ===")

    # Fetch new CVEs
    print("Fetching new CVEs from NVD...")
    new_cves = get_new_cves(days_back=7)
    print(f"Found {len(new_cves)} new CVEs")

    if not new_cves:
        print("No new CVEs to process")
        return

    # Get EPSS scores
    cve_ids = [v["cve"]["id"] for v in new_cves]
    print(f"Fetching EPSS scores for {len(cve_ids)} CVEs...")
    epss_scores = get_epss_scores(cve_ids)
    print(f"Got EPSS scores for {len(epss_scores)} CVEs")

    # Update Neo4j
    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
    try:
        print("Updating Neo4j taxonomy...")
        update_neo4j(new_cves, epss_scores, driver)
        print("âœ… Update complete")
    finally:
        driver.close()

if __name__ == "__main__":
    main()
```

### Update Impact on Entity Links

When new CVEs are added:
1. **Existing documents**: Links preserved (Entity â†’ old CVE)
2. **New documents**: Will automatically link to new CVEs via `link_entities_to_taxonomy.py`
3. **Re-ingestion**: Not required unless documents contain new CVE mentions

---

## 3. Why Does NER11 Crash? How Do We Fix It?

### Root Cause Analysis

**NER11 does NOT crash specifically on CVEs.** The crashes occur on **large documents (>70KB)** due to memory pressure in the Docker container.

#### Crash Pattern Evidence

| File | Size | Contains CVEs? | Crashes? |
|------|------|----------------|----------|
| PILOT_DEEP_ANALYSIS_STRATEGIC_INTELLIGENCE_REPORT.md | 106KB | Yes | âœ… Yes |
| iec62443-part4.md | 101KB | No | âœ… Yes |
| QTMP_Rail Cyberthreat Research_2025_10.md | 99KB | Yes | âœ… Yes |
| aeon-gtm-strategy.md | 97KB | No | âœ… Yes |
| Casper Sleep Inc. GTM Analysis_.md | ~80KB | No | âœ… Yes |

**Conclusion**: Crashes correlate with document SIZE, not CVE content.

### Why Large Documents Crash NER11

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NER11 MEMORY FLOW                             â”‚
â”‚                                                                  â”‚
â”‚  Large Document (100KB text)                                     â”‚
â”‚       â”‚                                                          â”‚
â”‚       â–¼                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ SpaCy Tokenization                                        â”‚   â”‚
â”‚  â”‚ - Creates token objects for every word                    â”‚   â”‚
â”‚  â”‚ - 100KB text â‰ˆ 15,000-20,000 tokens                      â”‚   â”‚
â”‚  â”‚ - Each token = ~500 bytes metadata                        â”‚   â”‚
â”‚  â”‚ - Total: ~10MB just for tokens                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                      â”‚
â”‚                           â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ NER Processing                                            â”‚   â”‚
â”‚  â”‚ - Transformer embeddings: 768-dim vectors per token       â”‚   â”‚
â”‚  â”‚ - 20,000 tokens Ã— 768 floats Ã— 4 bytes = ~60MB           â”‚   â”‚
â”‚  â”‚ - Attention matrices: O(nÂ²) memory                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                      â”‚
â”‚                           â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ âš ï¸  MEMORY EXHAUSTION                                     â”‚   â”‚
â”‚  â”‚ Docker container limit: typically 512MB-1GB               â”‚   â”‚
â”‚  â”‚ Peak usage: 200MB+ for large documents                    â”‚   â”‚
â”‚  â”‚ Result: OOM kill or connection reset                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Proposed Fixes

#### Fix 1: Document Chunking (Recommended)

Add chunking to `rate_limited_ingest.py`:

```python
def chunk_document(content: str, max_chunk_size: int = 50000) -> List[str]:
    """Split large documents into processable chunks."""
    if len(content) <= max_chunk_size:
        return [content]

    chunks = []
    paragraphs = content.split('\n\n')
    current_chunk = ""

    for para in paragraphs:
        if len(current_chunk) + len(para) < max_chunk_size:
            current_chunk += para + "\n\n"
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = para + "\n\n"

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks

def process_single_document(self, filepath: Path, doc_num: int, total: int) -> Dict:
    """Process document with chunking for large files."""
    content = filepath.read_text(encoding='utf-8', errors='ignore')

    # Chunk if necessary
    if len(content) > 50000:
        chunks = chunk_document(content)
        print(f"   ðŸ“„ Large doc split into {len(chunks)} chunks")

        all_entities = []
        for i, chunk in enumerate(chunks):
            entities = self.extract_entities(chunk)
            if entities:
                all_entities.extend(entities)
            time.sleep(1)  # Brief pause between chunks

        return self.store_entities(all_entities, filepath)
    else:
        entities = self.extract_entities(content)
        return self.store_entities(entities, filepath)
```

#### Fix 2: Increase Docker Memory Limits

```bash
# Check current NER11 container limits
docker inspect ner11-gold-api --format='{{.HostConfig.Memory}}'

# Restart with higher memory limit
docker stop ner11-gold-api
docker run -d --name ner11-gold-api \
    -p 8000:8000 \
    --memory=2g \
    --memory-swap=4g \
    ner11-gold-api:latest
```

#### Fix 3: Streaming NER Processing

Implement streaming in the NER11 API itself:

```python
# In NER11 API main.py
@app.post("/ner/stream")
async def extract_entities_stream(request: NERRequest):
    """Process document in streaming chunks."""
    text = request.text
    chunk_size = 10000  # Characters per chunk

    all_entities = []
    for i in range(0, len(text), chunk_size):
        chunk = text[i:i+chunk_size]
        # Ensure we don't split mid-sentence
        if i + chunk_size < len(text):
            last_period = chunk.rfind('.')
            if last_period > chunk_size * 0.8:
                chunk = chunk[:last_period+1]

        doc = nlp(chunk)
        entities = extract_from_doc(doc)
        all_entities.extend(entities)

        # Force garbage collection between chunks
        gc.collect()

    return {"entities": deduplicate(all_entities)}
```

---

## 4. Data Integrity Action Plan

### Immediate Actions (This Week)

| Action | Script | Priority |
|--------|--------|----------|
| Implement document chunking | Update `rate_limited_ingest.py` | ðŸ”´ High |
| Re-process skipped documents | Run with chunking enabled | ðŸ”´ High |
| Verify Entityâ†’CVE links | Run `link_entities_to_taxonomy.py` | ðŸŸ¡ Medium |

### Short-Term Actions (This Month)

| Action | Script | Priority |
|--------|--------|----------|
| Load CPE taxonomy | Create `load_cpe_taxonomy.py` | ðŸŸ¡ Medium |
| Add CVSS scores | Enhance CVE update script | ðŸŸ¡ Medium |
| Clean orphan CVE entities | Query and delete test CVEs | ðŸŸ¢ Low |

### Long-Term Actions (Quarterly)

| Action | Schedule | Priority |
|--------|----------|----------|
| Weekly CVE taxonomy updates | Cron job Sundays | ðŸŸ¡ Medium |
| Monthly EPSS refresh | Cron job 1st of month | ðŸŸ¢ Low |
| Quarterly full taxonomy audit | Manual review | ðŸŸ¢ Low |

---

## 5. Validation Queries

Use these Neo4j queries to monitor data integrity:

```cypher
// Check CVE coverage
MATCH (c:CVE)
WITH count(*) as total
MATCH (c:CVE) WHERE c.epss_score > 0
WITH total, count(*) as with_epss
RETURN total as total_cves, with_epss,
       round(100.0 * with_epss / total, 1) as epss_pct

// Check Entityâ†’CVE links
MATCH (e:Entity)-[r]->(c:CVE)
RETURN type(r) as relationship, count(*) as count

// Find orphan CVE entities (not in taxonomy)
MATCH (e:Entity)
WHERE e.text =~ '(?i)^CVE-[0-9]{4}-[0-9]+$'
WITH e, toUpper(e.text) as cve_id
OPTIONAL MATCH (c:CVE {id: cve_id})
WHERE c IS NULL
RETURN e.text as orphan_cve, e.source_file as source

// Check cross-taxonomy relationships
MATCH (c:CVE)-[r:HAS_WEAKNESS]->(w:CWE)
RETURN count(*) as cve_cwe_links

MATCH (w:CWE)-[r:ENABLES_ATTACK_PATTERN]->(a:CAPEC)
RETURN count(*) as cwe_capec_links
```

---

## Conclusion

**Your vulnerability data is fundamentally sound.** The architecture correctly separates structured taxonomy (CVE/CWE/CAPEC) from extracted entities. The low CVE entity count (49) is expected because:

1. Most ingested documents don't contain CVE references
2. CVE-heavy threat research documents crashed the API (now fixable with chunking)
3. The CVE Taxonomy (316K nodes) is the authoritative source, not entity extractions

**Key fixes needed:**
1. âœ… Implement document chunking to prevent crashes
2. âœ… Load CPE taxonomy to complete the picture
3. âœ… Set up weekly CVE updates from NVD

The system will function correctly once these improvements are in place.

---

**Report Generated**: 2025-12-02
**Session**: E30 Extended Analysis
**Grade**: Data Integrity B+ (gaps identified, fixes proposed)
